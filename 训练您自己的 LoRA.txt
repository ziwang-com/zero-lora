训练您自己的 LoRA
https://github.com/oobabooga/text-generation-webui/blob/main/docs/Training-LoRAs.md
训练您自己的 LoRA
WebUI 旨在使训练您自己的 LoRA 尽可能简单.它归结为几个简单的步骤：

第 1 步：制定计划。
您想使用哪种基本模型？您制作的 LoRA 必须与单个架构匹配 （例如 LLaMA-13B） 并且不能转移到其他人 （例如 LLaMA-7B， 稳定LM， 等等都是不同的）.同一模型的衍生物（例如LLaMA-13B的羊驼微调）可能是可转移的，但即便如此，最好完全按照您计划使用的内容进行训练。
您想要什么模型格式？在撰写本文时，8 位模型最稳定，支持 4 位模型，但处于实验阶段。在不久的将来，4位可能是大多数用户的最佳选择。
你在训练它什么？你想让它学习真实的信息，一个简单的格式，...？
步骤 2：收集数据集。
如果使用类似于 Alpaca 格式的数据集，则 WebUI 中的输入本机支持该数据集，并带有预制格式化程序选项。Formatted Dataset
如果使用的数据集与 Alpaca 的格式不匹配，但使用相同的基本 JSON 结构，则可以通过复制到新文件并编辑其内容来创建自己的格式化文件。training/formats/alpaca-format.json
如果您可以将数据集放入一个简单的文本文件中，那也可以！您可以使用输入选项进行训练。Raw text file
这意味着您可以复制/粘贴聊天记录/文档页面/任何您想要的内容，将其推入纯文本文件中，然后对其进行训练。
如果您使用的结构化数据集不是这种格式，则可能需要找到一种外部方法来转换它 - 或者打开问题以请求本机支持。
第 3 步：进行培训。
3.1：加载 WebUI 和模型。
确保您没有加载任何 LoRA（除非您想训练多 LoRA 使用）。
3.2：打开顶部的选项卡，子选项卡。TrainingTrain LoRA
3.3： 填写 LoRA 的名称， 在数据集选项中选择您的数据集.
3.4：根据您的喜好选择其他参数。请参阅下面的参数。
3.5：单击 ，然后等待。Start LoRA Training
大型数据集可能需要几个小时，如果进行小规模运行，则只需几分钟。
您可能希望在损失值发生时监控损失值。
第 4 步：评估您的结果。
在“模型”选项卡下加载 LoRA。
可以在选项卡上试用它，也可以使用选项卡的子选项卡。Text generationPerplexity evaluationTraining
如果您使用了该选项， 您可以从 LoRA 模型文件夹内的子文件夹中获取模型的先前副本并尝试它们.Save every n steps
第 5 步：如果您不满意，请重新运行。
确保在训练之前卸载 LoRA.
您可以简单地恢复先前的运行 - 用于选择 LoRA， 并编辑参数.请注意，您无法更改已创建的 LoRA.Copy parameters fromRank
如果要从沿途保存的检查点恢复， 只需将检查点文件夹的内容复制到 LoRA 的文件夹中.
（注意：是保存实际 LoRA 内容的重要文件）。adapter_model.bin
这将开始学习速率并返回到起点。如果您想像中途一样恢复，您可以将学习率调整为日志中上次报告的 LR 并减少您的 epoch。
或者，如果您愿意，可以完全重新开始。
如果您的模型产生损坏的输出，您可能需要重新开始并使用较低的学习率。
如果您的模型没有学习详细信息，但您希望它学习，则可能需要运行更多纪元，或者可能需要更高的排名。
如果你的模型强制执行你不想要的格式，你可能需要调整数据集，或者重新开始，而不是训练那么远。
格式化文件
如果使用 JSON 格式的数据集，则假定它们采用以下近似格式：

[
    {
        "somekey": "somevalue",
        "key2": "value2"
    },
    {
        // etc
    }
]
Where the keys (eg , above) are standardized, and relatively consistent across the dataset, and the values (eg , ) contain the content actually intended to be trained.somekeykey2somevaluevalue2

For Alpaca, the keys are , , and , wherein is sometimes blank.instructioninputoutputinput

A simple format file for Alpaca to be used as a chat bot is:

{
    "instruction,output": "User: %instruction%\nAssistant: %output%",
    "instruction,input,output": "User: %instruction%: %input%\nAssistant: %output%"
}
请注意，键（例如）是以逗号分隔的数据集键列表，值是将这些键与 .instruction,output%%

因此，例如，如果数据集具有 ，则格式化文件将自动填充为 ."instruction": "answer my question"User: %instruction%\nUser: answer my question\n

如果您有不同的键输入集，则可以创建自己的格式化文件来匹配它。此格式化文件设计为尽可能简单，以便轻松编辑以满足您的需求。

原始文本文件设置
使用原始文本文件作为数据集时，文本会根据你获得一些基本选项来配置它们，自动拆分为块。Cutoff Length

Overlap Length是块重叠的程度。重叠的块有助于防止模型学习奇怪的句子中间剪切，而是学习从早期文本流出的连续句子。
Prefer Newline Cut Length设置最大字符距离，以将块切口移向换行符。这样做有助于防止行在句子中间开始或结束，从而防止模型学习随机截断句子。
Hard Cut String设置一个字符串，指示必须存在没有重叠的硬剪切。这默认为 ，表示 3 个换行符。任何经过训练的块都不会包含此字符串。这允许您在同一文本文件中插入不相关的文本部分，但仍确保不会教模型随机更改主题。\n\n\n
参数
每个参数的基本用途和功能都记录在 WebUI 的页面上，因此请在 UI 中通读它们以了解您的选项。

也就是说，以下是您应该考虑的最重要参数选择的指南：

显温
首先，您必须考虑您的VRAM可用性。
通常， 在默认设置下， 使用默认参数进行训练的 VRAM 用法非常接近生成文本时 （具有 1000+ 上下文标记） （即， 如果您可以生成文本， 您可以训练 LoRA）.
注意：目前在 4 位猴子补丁中默认更糟。减少到以将其恢复到预期。Micro Batch Size1
如果您有多余的VRAM，设置更高的批量大小将使用更多的VRAM，并为您提供更高质量的培训作为交换。
如果您有大型数据，设置较高的截止长度可能是有益的，但会花费大量的 VRAM。如果可以留出一些，请将批量大小设置为 ，看看可以将截止长度提高多高。1
如果您的 VRAM 较低，减少批量大小或截止长度当然会改善这一点。
不要害怕尝试一下，看看会发生什么。如果太多，它只会出错，您可以降低设置并重试。
排
其次，你要考虑你想要的学习量。
例如，您可能希望只学习一种对话格式（如 Alpaca 的情况），在这种情况下，设置低值（32 或更低）效果很好。Rank
或者，你可能正在训练项目文档，希望机器人理解并能够理解有关问题，在这种情况下，排名越高越好。
一般来说，更高的等级=更精确的学习=更多的学习内容=更多的VRAM使用。
学习率和时期
第三，你希望它被学习得多么仔细。
换句话说，你对模型失去不相关的理解有多好。
您可以通过 3 个关键设置来控制这一点：学习率、其调度程序和您的总纪元。
学习率控制它看到的每个令牌对模型进行了多少更改。
它通常采用科学记数法，因此例如表示哪个是 .后面的数字控制数字中有多少个 s。3e-43 * 10^-40.0003e-0
值越高，训练运行得更快，但也更有可能损坏模型中的先前数据。
您基本上有两个变量需要平衡：LR 和纪元。
如果将 LR 设置得更高，则可以将纪元设置得同样低以匹配。高 LR + 低时期 = 非常快速、低质量的培训。
如果将 LR 设置为低，请将纪元设置为高。低 LR + 高时期 = 缓慢但高质量的培训。
调度程序在您训练时控制随时间的变化 - 它从高位开始，然后变低。这有助于在获取数据和同时保持良好的质量之间取得平衡。
您可以在HuggingFace文档中查看不同调度程序选项的图表 这里
损失
运行训练时，WebUI 的控制台窗口将记录报告，其中包括名为 的数值。它将从一个高数字开始，并逐渐变得越来越低。Loss

AI训练世界中的“损失”理论上意味着“模型离完美有多近”，意思是“绝对完美”。这是通过测量模型精确输出的训练它输出的文本与它实际输出的文本之间的差异来计算的。0

在实践中，一个好的LLM应该在其人工头中运行一个非常复杂的可变想法范围，因此丢失将表明模型已经损坏并且忘记了如何思考除您训练它之外的任何事情。0

因此，实际上，Loss是一个平衡游戏：您希望它足够低，以便它理解您的数据，但又足够高，以至于它不会忘记其他一切。一般来说，如果它低于，它将开始忘记它以前的记忆，你应该停止训练。在某些情况下，您可能更愿意将其降低到（如果您希望它非常非常可预测）。不同的目标有不同的需求，所以不要害怕尝试，看看什么最适合你。1.00.5

注意：如果您看到损失开始于或突然跳到确切，则可能是您的训练过程中出了问题（例如模型损坏）。0

注意：4 位猴子补丁
4 位 LoRA 猴子补丁适用于训练， 但有副作用：

VRAM使用率目前较高。您可以减少 to 进行补偿。Micro Batch Size1
模特做时髦的事情。LoRA 自己应用， 或拒绝应用， 或自发错误， 或等.在训练/使用之间重新加载基本模型或重新启动 WebUI 可能会有所帮助，以最大程度地减少出现任何问题的可能性。
同时加载或使用多个 LoRA 目前不起作用.
一般来说，识别猴子补丁并将其视为肮脏的临时黑客 - 它可以工作，但不是很稳定。当所有内容都合并到上游以获得全面的官方支持时，它会及时变得更好。
旧版笔记
LoRA 培训由 mcmonkey4eva 在 PR #570 中提供。

使用原始羊驼-洛拉代码
保留在这里以供参考。“训练”选项卡具有比此方法更多的功能。

conda activate textgen
git clone https://github.com/tloen/alpaca-lora
编辑这两行以使用现有的模型文件夹，而不是从十足点下载所有内容：alpaca-lora/finetune.py

model = LlamaForCausalLM.from_pretrained(
    "models/llama-7b",
    load_in_8bit=True,
    device_map="auto",
)
tokenizer = LlamaTokenizer.from_pretrained(
    "models/llama-7b", add_eos_token=True
)
使用以下命令运行脚本：

python finetune.py
它只是工作。它以 22.32s/it 的速度运行， 总共有 1170 次迭代， 所以大约 7 训练 LoRA 的一个半小时.使用RTX 3090，18153MiB VRAM，消耗最大功率（350W，房间加热器模式）。